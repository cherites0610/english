import React, { useState, useEffect, useRef } from "react";
import {
  SafeAreaView,
  StyleSheet,
  Text,
  View,
  Button,
  Platform,
  ActivityIndicator,
  ImageBackground,
  TouchableOpacity,
  Image,
} from "react-native";
import * as FileSystem from "expo-file-system";
import {
  useAudioRecorder,
  RecordingConfig,
  ExpoAudioStreamModule,
} from "@siteed/expo-audio-studio";
import TrackPlayer, {
  State as TrackPlayerState,
  Event as TrackPlayerEvent,
  useTrackPlayerEvents,
} from "react-native-track-player";
import { io, Socket } from "socket.io-client";
import Rive, { RiveRef } from "rive-react-native";

// åœ¨ App æª”æ¡ˆé ‚éƒ¨è¨»å†Šæ’­æ”¾æœå‹™
TrackPlayer.registerPlaybackService(() => require("./service.js"));

// --- App Component ---
export default function App() {
  // --- éŒ„éŸ³ç›¸é—œ ---
  const { startRecording, stopRecording, isRecording } = useAudioRecorder();
  const audioChunksRef = useRef<string[]>([]);

  const riveRef = useRef<RiveRef>(null);

  // --- ç‹€æ…‹ç®¡ç† ---
  const [isConnected, setIsConnected] = useState<boolean>(false);
  const [statusText, setStatusText] = useState<string>("æ­£åœ¨é€£ç·šåˆ°ä¼ºæœå™¨...");
  const [isConversationEnded, setIsConversationEnded] =
    useState<boolean>(false);
  const [isAiSpeaking, setIsAiSpeaking] = useState<boolean>(false);

  const [isInitialized, setIsInitialized] = useState<boolean>(true);

  // --- Ref ç®¡ç† ---
  const socketRef = useRef<Socket | null>(null);
  const hasSentGreetingRef = useRef<boolean>(false);

  const SERVER_URL =
    Platform.OS === "android"
      ? "http://10.0.2.2:5010"
      : "https://61def6c3e8a3.ngrok-free.app";

  useEffect(() => {
    // é€™å€‹ effect æœƒç›£è½ isRecording çš„è®ŠåŒ–
    console.log(`Rive: Syncing 'listening' state to: ${isRecording}`);
    riveRef.current?.setInputState("State Machine 1", "listening", isRecording);
  }, [isRecording]); // <--- ä¾è³´é …é™£åˆ—ä¸­åªæœ‰ isRecording

  // ã€æ–°å¢ã€‘Effect 2: åŒæ­¥ã€ŒAI èªªè©±ç‹€æ…‹ã€åˆ° Rive
  useEffect(() => {
    // é€™å€‹ effect æœƒç›£è½ isAiSpeaking çš„è®ŠåŒ–
    console.log(`Rive: Syncing 'talk' state to: ${isAiSpeaking}`);
    riveRef.current?.setInputState("State Machine 1", "talk", isAiSpeaking);
  }, [isAiSpeaking]);

  // --- Track Player åˆå§‹åŒ–è¨­å®š ---
  useEffect(() => {
    const setupPlayer = async () => {
      try {
        await TrackPlayer.setupPlayer();
        await TrackPlayer.updateOptions({
          capabilities: [],
          compactCapabilities: [],
        });
      } catch (error) {
        console.error("Error setting up Track Player:", error);
      }
    };
    setupPlayer();

    return () => {
      TrackPlayer.reset();
    };
  }, []);

  // --- WebSocket é€£ç·šèˆ‡äº‹ä»¶ç›£è½ ---
  useEffect(() => {
    console.log("Setting up socket connection ONCE.");
    const socket = io(SERVER_URL);
    socketRef.current = socket;

    const onConnect = () => {
      console.log(`âœ… Connected to WebSocket! My ID: ${socket.id}`);
      setIsConnected(true);
      setStatusText("å·²é€£ç·šï¼Œç­‰å¾… AI é–‹å ´ç™½...");

      if (!hasSentGreetingRef.current) {
        const characterName = "Elara"; // å¯æ›¿æ›ç‚ºè®Šæ•¸
        console.log(`ğŸš€ Sending createConversation for name: ${characterName}`);
        socket.emit("createConversation", { name: characterName });
        hasSentGreetingRef.current = true;
      }
    };

    const onDisconnect = () => {
      console.log("ğŸ”Œ Disconnected from WebSocket server.");
      setIsConnected(false);
      setStatusText("å·²å¾ä¼ºæœå™¨æ–·ç·š");
    };

    const onAudioResponse = (audioChunk: string) => {
      if (audioChunk) {
        console.log("æ¥æ”¶åˆ°éŸ³è¨Šï¼");

        audioChunksRef.current.push(audioChunk);
      }
    };

    const onFinalResponse = (data: { text: string }) => {
      console.log(`ğŸ Received FINAL text response: "${data.text}"`);
      // æ›´æ–°ç‹€æ…‹æ–‡å­—ï¼Œé¡¯ç¤ºæœ€å¾Œçš„å›æ‡‰
      setStatusText(data.text);
      // å°‡å°è©±æ¨™è¨˜ç‚ºå·²çµæŸ
      setIsConversationEnded(true);
      // ç¢ºä¿ AI èªªè©±ç‹€æ…‹çµæŸ
      setIsAiSpeaking(false);
    };

    const onEndAudioResponse = () => {
      console.log(
        "ğŸ Received end of audio signal. Processing full response..."
      );

      const chunks = audioChunksRef.current;
      if (chunks.length === 0) {
        console.log("Chunks ref is empty, nothing to play.");
        if (hasSentGreetingRef.current && !isRecording) {
          setStatusText("è¼ªåˆ°ä½ äº†ï¼Œè«‹æŒ‰éˆ•é–‹å§‹éŒ„éŸ³");
        }
        return;
      }

      // å–å¾—è³‡æ–™å¾Œï¼Œç«‹åˆ»æ¸…ç©º refï¼Œç‚ºä¸‹ä¸€æ¬¡å°è©±åšæº–å‚™
      audioChunksRef.current = [];

      const playFullResponseFromFile = async (queue: string[]) => {
        try {
          console.log(
            `Concatenating and saving ${queue.length} audio chunks...`
          );
          const fullBase64 = queue.join("");
          const uri =
            FileSystem.cacheDirectory + `full_response_${Date.now()}.mp3`;
          console.log("uri:", uri);

          await FileSystem.writeAsStringAsync(uri, fullBase64, {
            encoding: FileSystem.EncodingType.Base64,
          });

          await TrackPlayer.reset();
          await TrackPlayer.add({
            url: uri,
            title: "AI Full Response",
            artist: "Assistant",
          });
          await TrackPlayer.play();
        } catch (error) {
          console.error("ERROR during final playback setup", error);
        }
      };

      playFullResponseFromFile(chunks);
    };

    socket.on("connect", onConnect);
    socket.on("disconnect", onDisconnect);
    socket.on("audioResponse", onAudioResponse);
    socket.on("endAudioResponse", onEndAudioResponse);
    socket.on("finalResponse", onFinalResponse);

    return () => {
      console.log("Cleaning up socket connection for ID:", socket.id);
      socket.off("connect", onConnect);
      socket.off("disconnect", onDisconnect);
      socket.off("audioResponse", onAudioResponse);
      socket.off("endAudioResponse", onEndAudioResponse);
      socket.off("finalResponse", onFinalResponse);
      socket.disconnect();
    };
  }, []);

  // --- Hook: ç›£è½ Track Player äº‹ä»¶ä»¥æ›´æ–° UI ---
  useTrackPlayerEvents(
    [TrackPlayerEvent.PlaybackState, TrackPlayerEvent.PlaybackError], // <-- æ–°å¢ Event.PlaybackError
    (event) => {
      if (event.type === TrackPlayerEvent.PlaybackState) {
        const isSpeaking =
          event.state === TrackPlayerState.Playing ||
          event.state === TrackPlayerState.Buffering;
        setIsAiSpeaking(isSpeaking);

        if (isSpeaking) {
          setStatusText("AI æ­£åœ¨èªªè©±...");
        } else {
          if (!isRecording && hasSentGreetingRef.current) {
            setStatusText("è¼ªåˆ°ä½ äº†ï¼Œè«‹æŒ‰éˆ•é–‹å§‹éŒ„éŸ³");
          }
        }
      }
    }
  );

  // --- éŒ„éŸ³æ§åˆ¶å‡½å¼ ---
  const handleStart = async () => {
    try {
      await ExpoAudioStreamModule.requestPermissionsAsync();
      setStatusText("æ­£åœ¨è†è½...");

      const config: RecordingConfig = {
        interval: 250,
        sampleRate: 16000,
        channels: 1,
        encoding: "pcm_16bit",
        onAudioStream: async (audioData) => {
          if (socketRef.current?.connected && audioData.data) {
            socketRef.current.emit("audioStream", audioData.data);
          }
        },
        output: { primary: { enabled: false } },
      };
      await startRecording(config);
    } catch (error) {
      console.error("Failed to start recording:", error);
      setStatusText("éŒ„éŸ³å•Ÿå‹•å¤±æ•—");
    }
  };

  const handleStop = async () => {
    await stopRecording();
    if (socketRef.current?.connected) {
      socketRef.current.emit("endAudioStream");
    }
    await TrackPlayer.reset(); // å¼·åˆ¶åœæ­¢ä¸¦æ¸…ç©º AI æœªèªªå®Œçš„è©±
    setStatusText("AI æ­£åœ¨æ€è€ƒ...");
  };

  // --- UI æ¸²æŸ“ ---
  const renderLoadingScreen = () => (
    <View style={styles.loadingContainer}>
      <ActivityIndicator size="large" color="#FFFFFF" />
      <Text style={styles.loadingText}>INITIALIZING...</Text>
    </View>
  );

  const renderisFinishScreen = () => (
    <View style={styles.loadingContainer}>
      <ActivityIndicator size="large" color="#FFFFFF" />
      <Text style={styles.loadingText}>å°è©±å·²ç¶“çµæŸ</Text>
    </View>
  );

  // --- å°è©±ä»‹é¢ ---
  const renderChatScreen = () => (
    <ImageBackground
      source={require("@/assets/images/Dialogue/bg.png")}
      resizeMode="cover"
      style={styles.chatContainer}
    >
      <SafeAreaView style={styles.safeArea}>
        <View style={styles.characterContainer}>
          <Rive
            ref={riveRef}
            url="https://mou-english.s3.ap-northeast-1.amazonaws.com/Anna.riv"
            artboardName="iPhone 16 - 1"
            stateMachineName="State Machine 1"
            autoplay={true}
            style={styles.rive}
          />
          <Image
            source={require("@/assets/images/Dialogue/fg.png")}
            style={styles.overlayImage}
          />
        </View>

        {/* éŒ„éŸ³æŒ‰éˆ• */}
        <TouchableOpacity
          style={[
            styles.recordButton,
            (isRecording || isAiSpeaking) && styles.recordButtonDisabled, // éŒ„éŸ³æˆ– AI èªªè©±æ™‚ï¼ŒæŒ‰éˆ•è®ŠåŠé€æ˜
            isRecording && styles.recordButtonActive, // éŒ„éŸ³æ™‚ï¼ŒæŒ‰éˆ•åŠ å€‹å¤–æ¡†
          ]}
          onPress={isRecording ? handleStop : handleStart}
          disabled={!isConnected || isAiSpeaking} // AI èªªè©±æ™‚å®Œå…¨ç¦ç”¨æŒ‰éˆ•
        >
          <Image
            source={require("@/assets/images/Dialogue/fg.png")}
            style={styles.micIcon}
          />
        </TouchableOpacity>
      </SafeAreaView>
    </ImageBackground>
  );

  return isInitialized
    ? isConversationEnded
      ? renderisFinishScreen()
      : renderChatScreen()
    : renderLoadingScreen();
}

// --- æ¨£å¼ ---
const styles = StyleSheet.create({
  // --- è¼‰å…¥ä»‹é¢æ¨£å¼ ---
  loadingContainer: {
    flex: 1,
    justifyContent: "center",
    alignItems: "center",
    backgroundColor: "#1c1c1e", // æ·±è‰²èƒŒæ™¯
  },
  loadingText: {
    marginTop: 20,
    fontSize: 18,
    color: "#FFFFFF",
    fontWeight: "bold",
  },

  // --- å°è©±ä»‹é¢æ¨£å¼ ---
  chatContainer: {
    flex: 1,
  },
  safeArea: {
    flex: 1,
    justifyContent: "center",
    alignItems: "center",
  },

  // --- äººç‰©ç›¸é—œæ¨£å¼ ---
  characterContainer: {
    // ä½¿ç”¨çµ•å°å®šä½è®“å®¹å™¨ç–Šåœ¨èƒŒæ™¯åœ–ä¸Šï¼Œä¸¦ä½”æ»¿å…¨è¢å¹•
    position: "absolute",
    top: 0,
    left: 0,
    right: 0,
    bottom: 0,
    marginTop: 100,
    justifyContent: "center",
    alignItems: "center",
  },
  characterImage: {
    width: "100%",
    height: "100%",
    resizeMode: "contain",
  },
  overlayImage: {
    width: "100%",
    height: "100%",
    resizeMode: "contain",
    position: "absolute",
    marginTop: 200,
  },

  // --- éŒ„éŸ³æŒ‰éˆ•æ¨£å¼ ---
  recordButton: {
    position: "absolute",
    bottom: 60, // è·é›¢åº•éƒ¨ 60px
    width: 80,
    height: 80,
    borderRadius: 40, // åœ“å½¢
    backgroundColor: "rgba(255, 255, 255, 0.3)",
    justifyContent: "center",
    alignItems: "center",
    borderWidth: 2,
    borderColor: "rgba(255, 255, 255, 0.5)",
  },
  recordButtonActive: {
    // æ­£åœ¨éŒ„éŸ³æ™‚çš„æ¨£å¼ï¼Œä¾‹å¦‚ç™¼å…‰çš„ç´…è‰²å¤–æ¡†
    borderColor: "#ff453a",
    backgroundColor: "rgba(255, 69, 58, 0.4)",
  },
  recordButtonDisabled: {
    opacity: 0.5, // ç¦ç”¨æ™‚è®ŠåŠé€æ˜
  },
  rive: {
    width: 400,
    height: 400,
  },
  micIcon: {
    width: 40,
    height: 40,
    resizeMode: "contain",
  },
});
