import {
  OnGatewayConnection,
  OnGatewayDisconnect,
  OnGatewayInit,
  SubscribeMessage,
  WebSocketGateway,
  ConnectedSocket,
  MessageBody,
  WebSocketServer,
} from '@nestjs/websockets';
import { Logger, NotFoundException } from '@nestjs/common';
import { Server, WebSocket } from 'ws';
import { SpeechClient } from '@google-cloud/speech';
import * as fs from 'fs';
import {
  GoogleGenerativeAI,
  Part,
  GenerativeModel,
} from '@google/generative-ai';
import { RedisService } from 'src/redis/redis.service';
import { BattleAdminService } from 'src/battle/battle-admin.service';
import { UserService } from 'src/user/user.service';
import { v4 as uuidv4 } from 'uuid';

interface CreateConversationPayload {
  name: string; // ÂèØËÉΩÊòØ npcName Êàñ battleName
  userID: string;
}

interface DialogueAnalysisResult {
  summary: string;
  grammar: string;
  keywords: {
    word: string;
    explanation: string;
  }[];
}

// --- ÂûãÂà•ÂÆöÁæ© ---
interface ChatHistory {
  role: 'user' | 'model';
  parts: Part[];
}

// --- Gemini Á≥ªÁµ±Êåá‰ª§ ---
const GEMINI_SYSTEM_INSTRUCTION = `
# Role: Conversational Lifestyle Robot

## Profile
- language: English
- description: AI for natural, engaging conversations, adapting linguistic style to predefined personas. Simulates human-like interactions.
- background: Simulates social scenarios and relationships.
- personality: Customizable via "Persona Configuration" (witty/sarcastic to friendly/supportive).
- expertise: Conversational English, contextual understanding, natural language generation, scenario-based dialogue.
- target_audience: Users seeking personalized AI conversations; developers needing versatile conversational AI.

## Skills

1. Core Conversational Skills
   - NLG: Produces fluent, contextually appropriate responses.
   - Contextual Understanding: Interprets user input to maintain relevance.
   - Sentiment Analysis: Responds to emotional undertones.
   - Persona Emulation: Embodies personality, background, and English proficiency from Persona Configuration.

2. Technical Skills
   - Variable Integration: Integrates traits, proficiency, background, and ending conditions from Persona Configuration.
   - Ending Condition Detection: Identifies predefined conversation conclusions.
   - Error Handling: Manages errors gracefully.

## Rules

1. Basic Principles:
   - Persona Adherence: Adhere to personality, English proficiency, background, and ending condition in Persona Configuration.
   - Conciseness: Provide direct responses.
   - Language Consistency: Maintain consistent English level.
   - Ending Condition Tagging: Append "[EOC]" if ending condition met; otherwise, "[EOT]".

2. Behavior Guidelines:
   - Contextual Appropriateness: Responses relevant to background and conversation.
   - Adaptive Language: Adjust language to match English proficiency.
   - Personality Consistency: Uphold assigned personality traits.
   - Ending Condition Monitoring: Monitor for predefined ending condition.

3. Limitations:
   - No External Data: Do not access external data.
   - Focused Scope: Generate responses and detect ending condition only.
   - No Code Blocks: Use plain text only.

## Workflows

- Goal: Generate responses based on Persona Configuration and determine if ending condition is met.
- Step 1: Analyze user input, adhering to Persona Configuration.
- Step 2: Construct response aligning with personality, English proficiency, and background.
- Step 3: Evaluate user input for ending condition.
- Step 4: Generate response and append "[EOC]" or "[EOT]".
- Expected Result: Conversational response with appropriate tag.

## OutputFormat

1. Text Output:
   - format: Plain text
   - style: Natural, conversational
   - special_requirements: Response with "[EOC]" or "[EOT]".

2. Format Specifications:
   - indentation: N/A
   - sections: Conversational text and ending tag only.
   - highlighting: N/A

3. Validation Rules:
   - constraints: Coherent sentences ending with "[EOC]" or "[EOT]".

4. Examples:
   1. Example 1:
      - Title: Standard Response
      - Format Type: Text
      - Description: Ending condition not met.
      - Example Content: What's new with you?[EOT]
   
   2. Example 2:
      - Title: Conversation Ending
      - Format Type: Text
      - Description: User signaled end.
      - Example Content: See you later![EOC]

## Initialization
As a Conversational Lifestyle Robot, adhere to Rules, execute Workflows, and output responses in plain text with the appropriate tag. Persona is defined in Persona Configuration.
`;

@WebSocketGateway({
  cors: { orgin: '*' },
})
export class TalkGateway
  implements OnGatewayInit, OnGatewayConnection, OnGatewayDisconnect
{
  @WebSocketServer()
  server: Server;

  private readonly logger = new Logger('TalkGateway');
  private readonly speechClient: SpeechClient;
  private readonly genAI: GoogleGenerativeAI;
  private readonly geminiModel: GenerativeModel;

  private readonly speechStreams = new Map<string, any>();
  private readonly ttsConnections = new Map<string, WebSocket>();

  private clients: Map<WebSocket, string> = new Map();
  private clientsById: Map<string, WebSocket> = new Map();

  constructor(
    private readonly redisService: RedisService,
    private readonly userService: UserService,
    private readonly battleService: BattleAdminService,
  ) {
    const gcpCredentialsJsonBase64 = process.env.GCP_CREDENTIALS_JSON;
    if (!gcpCredentialsJsonBase64)
      throw new Error('GCP credentials are required.');
    const gcpCredentialsJsonBuffer = Buffer.from(
      gcpCredentialsJsonBase64,
      'base64',
    ).toString('utf-8');
    const credentials = JSON.parse(gcpCredentialsJsonBuffer);
    this.speechClient = new SpeechClient({ credentials });

    const geminiApiKey = process.env.GCP_GEMINI_API;
    if (!geminiApiKey) throw new Error('GeminiAPI credentials are required.');
    this.genAI = new GoogleGenerativeAI(geminiApiKey);

    this.geminiModel = this.genAI.getGenerativeModel({
      model: 'gemini-1.5-flash',
      systemInstruction: GEMINI_SYSTEM_INSTRUCTION,
    });
  }

  afterInit(server: any) {
    this.logger.log('‚úÖ [Á≥ªÁµ±] WebSocket ÈñòÈÅìÂô®Â∑≤ÂàùÂßãÂåñÔºÅ');
  }

  handleConnection(client: WebSocket) {
    const clientId = uuidv4();
    this.clients.set(client, clientId);
    this.clientsById.set(clientId, client);
    this.logger.log(`üîó [${clientId}] ÂÆ¢Êà∂Á´ØÈÄ£Á∑öÊàêÂäü„ÄÇ`);

    client.on('message', (data: Buffer) => {
      if (data instanceof Buffer) {
        if (!this.speechStreams.has(clientId)) {
          this.createNewStreamForClient(clientId);
        }
        this.speechStreams.get(clientId)?.write(data);
      }
    });
  }

  handleDisconnect(client: WebSocket) {
    const clientId = this.clients.get(client);
    if (clientId) {
      this.logger.log(`üîå [${clientId}] ÂÆ¢Êà∂Á´ØÈÄ£Á∑ö‰∏≠Êñ∑„ÄÇ`);
      this.closeStreamForClient(clientId);
      this.ttsConnections.get(clientId)?.close();
      this.clients.delete(client);
      this.clientsById.delete(clientId);
    }
  }

  @SubscribeMessage('createConversation')
  async handleCreateConversation(
    @ConnectedSocket() client: WebSocket,
    @MessageBody() payload: CreateConversationPayload,
  ) {
    const clientId = this.clients.get(client);
    if (!clientId) return;
    this.logger.log(
      `üöÄ [${clientId}] Êî∂Âà∞ 'createConversation' ‰∫ã‰ª∂ÔºåÁõÆÊ®ôÔºö'${payload.name}'`,
    );
    const initialPrompt = await this.buildInitialPrompt(
      payload.name,
      payload.userID,
    );
    await this.processAndStreamResponse(initialPrompt, clientId, []);
  }

  @SubscribeMessage('endAudioStream')
  handleEndAudioStream(@ConnectedSocket() client: WebSocket) {
    const clientId = this.clients.get(client);
    if (!clientId) return;
    const stream = this.speechStreams.get(clientId);
    if (stream) {
      this.logger.log(`üé§ [${clientId}] Êî∂Âà∞Èü≥Ë®ä‰∏≤ÊµÅÁµêÊùü‰ø°ËôüÔºåÂÅúÊ≠¢ STT Ëº∏ÂÖ•„ÄÇ`);
      stream.end();
    }
  }

  private async buildInitialPrompt(
    name: string,
    userId: string,
  ): Promise<string> {
    this.logger.debug(`[Á≥ªÁµ±] Ê≠£Âú®ÁÇ∫ÁõÆÊ®ô '${name}' Âª∫Á´ãÂàùÂßãÂ∞çË©±ÊèêÁ§∫...`);
    const user = await this.userService.findByID(userId);
    if (!user) {
      throw new NotFoundException(`User with ID "${userId}" not found`);
    }
    const battle =
      await this.battleService.getRandomStageByChildCategoryName(name);
    if (!battle) {
      throw new NotFoundException(`Battle with Name "${name}" not found`);
    }
    const prompt = `Ë´ã‰Ω†ÊâÆÊºî‰∏Ä‰ΩçÂêçÂè´'${battle.npc?.name}'ÔºåÁèæÂú®ÁöÑÊïÖ‰∫ãÁÇ∫${battle.backstory},ÈúÄË¶Å‰ΩøÁî®ÁöÑËã±ÊñáÈõ£Â∫¶ÁÇ∫${user.englishLevel},ÁõÆÊ®ôÁÇ∫${battle.targets},Ë´ãÈñãÂßã‰Ω†ÁöÑÂ∞çË©±ÔºÅ`;
    return prompt;
  }

  private createNewStreamForClient(clientId: string) {
    this.logger.log(`üé§ [${clientId}] Ê≠£Âú®Âª∫Á´ãÊñ∞ÁöÑ STT Ë™ûÈü≥Ë≠òÂà•‰∏≤ÊµÅ„ÄÇ`);
    const stream = this.speechClient
      .streamingRecognize({
        config: {
          encoding: 'LINEAR16',
          sampleRateHertz: 44100,
          languageCode: 'en-us',
          enableAutomaticPunctuation: true,
        },
        interimResults: false,
      })
      .on('error', (err) => {
        this.logger.error(`‚ùå [${clientId}] STT ‰∏≤ÊµÅÈåØË™§:`, err);
        this.closeStreamForClient(clientId);
      })
      .on('data', async (data) => {
        const transcript = data.results[0]?.alternatives[0]?.transcript;
        if (transcript && data.results[0].isFinal) {
          this.logger.log(`üé§ [${clientId}] STT ÊúÄÁµÇË≠òÂà•ÁµêÊûú: '${transcript}'`);
          const history = await this.getHistoryFromRedis(clientId);
          await this.processAndStreamResponse(transcript, clientId, history);
          this.closeStreamForClient(clientId);
        }
      });
    this.speechStreams.set(clientId, stream);
  }

  private closeStreamForClient(clientId: string) {
    const stream = this.speechStreams.get(clientId);
    if (stream) {
      this.logger.log(`üßπ [${clientId}] Ê≠£Âú®Ê∏ÖÁêÜ STT Ë™ûÈü≥Ë≠òÂà•‰∏≤ÊµÅ...`);
      stream.destroy();
      this.speechStreams.delete(clientId);
    }
  }

  private async processAndStreamResponse(
    prompt: string,
    clientId: string,
    history: ChatHistory[],
  ) {
    this.logger.log(`ü§ñ [${clientId}] ÈñãÂßãËôïÁêÜÊèêÁ§∫‰∏¶‰∏≤ÊµÅÂõûË¶Ü...`);
    this.logger.debug(`[${clientId}] ÂæÖËôïÁêÜÁöÑÊèêÁ§∫ÊñáÂ≠ó: "${prompt}"`);

    let geminiStreamFinished = false;
    let ttsStreamClosed = false;

    const checkAndFinalize = () => {
      if (geminiStreamFinished && ttsStreamClosed) {
        this.logger.log(
          `‚úÖ [${clientId}] Gemini Ëàá TTS ‰∏≤ÊµÅÂùáÂ∑≤ÂÆåÊàêÔºåÁôºÈÄÅÊúÄÁµÇÁµêÊùü‰ø°Ëôü„ÄÇ`,
        );
        this.sendToClientById(clientId, 'endAudioResponse', {});
      }
    };

    try {
      this.startElevenLabsStream(clientId, () => {
        ttsStreamClosed = true;
        checkAndFinalize();
      });

      const chat = this.geminiModel.startChat({ history });
      const result = await chat.sendMessageStream(prompt);
      const ttsWs = this.ttsConnections.get(clientId);
      let fullResponse = '';
      let endCondition: 'EOT' | 'EOC' | null = null;

      for await (const chunk of result.stream) {
        let chunkText = chunk.text();
        // „Äê‰øÆÊ≠£„Äë‰ΩøÁî® if...else if ‰æÜÁ¢∫‰øù [EOC] ÁöÑÂÑ™ÂÖàÁ¥ö
        if (chunkText.includes('[EOC]')) {
          endCondition = 'EOC';
          chunkText = chunkText.replace('[EOC]', '');
        } else if (chunkText.includes('[EOT]')) {
          endCondition = 'EOT';
          chunkText = chunkText.replace('[EOT]', '');
        }
        if (chunkText) {
          fullResponse += chunkText;
          if (ttsWs?.readyState === WebSocket.OPEN) {
            ttsWs.send(JSON.stringify({ text: chunkText }));
          }
        }
        if (endCondition) break;
      }

      geminiStreamFinished = true;
      this.logger.log(`‚úÖ [${clientId}] Gemini ‰∏≤ÊµÅÂ∑≤ÂÆåÊàê„ÄÇ`);
      this.logger.debug(`[${clientId}] Gemini ÂÆåÊï¥ÂõûË¶Ü: "${fullResponse}"`);

      if (ttsWs?.readyState === WebSocket.OPEN) {
        ttsWs.send(JSON.stringify({ text: '' }));
      }
      checkAndFinalize();

      if (endCondition === 'EOC') {
        this.logger.log(
          `üèÅ [${clientId}] ÂÅµÊ∏¨Âà∞Â∞çË©±ÁµêÊùüÊ¢ù‰ª∂ [EOC]„ÄÇÊ∫ñÂÇôÈÄ≤Ë°åÊúÄÁµÇÂàÜÊûê„ÄÇ`,
        );
        this.sendToClientById(clientId, 'finalResponse', {
          text: fullResponse,
        });
        this.processFinalConversation(clientId, history, prompt, fullResponse); // ÈùûÂêåÊ≠•Âü∑Ë°å
        await this.redisService.del(this.getHistoryRedisKey(clientId));
      } else {
        this.logger.log(`üí¨ [${clientId}] Â∞çË©±ÁπºÁ∫å [EOT]„ÄÇÊ≠£Âú®Êõ¥Êñ∞Ê≠∑Âè≤Á¥ÄÈåÑ„ÄÇ`);
        await this.updateHistoryInRedis(
          clientId,
          history,
          { role: 'user', parts: [{ text: prompt }] },
          { role: 'model', parts: [{ text: fullResponse }] },
        );
      }
    } catch (error) {
      this.logger.error(`‚ùå [${clientId}] ËôïÁêÜ‰∏≤ÊµÅÂõûË¶ÜÊôÇÁôºÁîüÈåØË™§:`, error);
      this.ttsConnections.get(clientId)?.close();
    }
  }

  private async processFinalConversation(
    clientId: string,
    history: ChatHistory[],
    lastUserPrompt: string,
    lastModelResponse: string,
  ) {
    this.logger.log(`üìä [${clientId}] ÈñãÂßãÂ∞çÂÆåÊï¥Â∞çË©±ÈÄ≤Ë°åÊúÄÁµÇÂàÜÊûê...`);
    try {
      let finalConversation = history
        .map((turn) => `${turn.role}: ${turn.parts[0].text}`)
        .join('\n');
      finalConversation += `\nuser: ${lastUserPrompt}`;
      finalConversation += `\nmodel: ${lastModelResponse}`;

      this.logger.debug(
        `[${clientId}] üìú Áî®ÊñºÊúÄÁµÇÂàÜÊûêÁöÑÂÆåÊï¥Â∞çË©±Êó•Ë™å:\n${finalConversation}`,
      );

      const finalPrompt = `
# Role: AI Dialogue Analysis Expert

## Profile
- language: English
- description: Analyzes and summarizes user-AI dialogues, focusing on semantics, grammar, and key words, outputting in JSON.
- background: NLP, speech recognition, and data analysis expertise.
- personality: Precise, objective, practical, and efficient.
- expertise: Dialogue analysis, semantic understanding, grammar analysis, keyword extraction, JSON formatting.
- target_audience: Speech tech developers, language learners, AI product managers.

## Skills

1. Dialogue Analysis and Summarization
   - Semantic Understanding: Accurately understand dialogue meaning.
   - Content Extraction: Extract main information and viewpoints.
   - Summarization: Concisely summarize dialogue (within 20 seconds of speech).
   - Intent Recognition: Identify user and AI communication intentions.

2. Grammar Analysis
   - Grammar Correction: Identify potential grammatical errors.
   - Structural Analysis: Analyze sentence structure (subject-verb-object, etc.).
   - Usage Explanation: Explain grammatical phenomena.

3. Keyword Extraction
   - Key Word Identification: Identify important words/phrases.
   - Contextual Association: Infer word meaning from context.
   - Part-of-Speech Tagging: Tag word part of speech.
   - Meaning Explanation: Explain key word meaning in dialogue.

4. JSON Formatting
   - Data Structure Design: Design compliant JSON structure.
   - Format Conversion: Convert analysis results to JSON.
   - Format Validation: Ensure JSON correctness.
   - Encoding Handling: Handle encoding for correct parsing.

## Rules

1. Basic Principles:
   - Accuracy: Ensure analysis accuracy.
   - Objectivity: Maintain objectivity and neutrality.
   - Simplicity: Concise summary content.
   - Comprehensiveness: Cover key dialogue information.

2. Behavioral Guidelines:
   - Context Priority: Consider context during analysis.
   - Hypothesis Verification: Verify assumptions about speech recognition errors.
   - Emphasis: Highlight key information.
   - Continuous Learning: Improve analysis capabilities.

3. Constraints:
   - Duration Limit: Summarized speech around 20 seconds.
   - Error Tolerance: Handle speech recognition errors.
   - Knowledge Boundary: Acknowledge knowledge limitations.
   - Avoid Creation: Only analyze existing dialogue.

## Workflows

- Goal: Analyze user-AI dialogue and output JSON with summaries, grammar analysis, and key words. Input and output in English.
- Step 1: Receive user-AI dialogue in English.
- Step 2: Perform semantic understanding and content extraction, focusing on user intent.
- Step 3: Perform grammar analysis, identifying and correcting errors.
- Step 4: Extract key words and explain their meanings.
- Step 5: Organize information into JSON format.
- Step 6: Output compliant JSON data.
- Expected Result: JSON file for voice assistants, language learning, etc.

## OutputFormat

1. JSON Output Format:
   - format: json
   - structure: Contains three top-level fields: 'summary', 'grammar', 'keywords'.
     - 'summary': String, containing the summary of the dialogue.
     - 'grammar': String, containing the grammar summary of the dialogue.
     - 'keywords': Array, containing objects of key single words. Each object contains 'word' (single word) and 'explanation' (explanation). 'keywords' is a required field, and should be an empty array "[]" if no keywords are identified.
   - style: Concise, clear, easy to parse.
   - special_requirements: Ensure all characters use UTF-8 encoding.

2. Format Specifications:
   - indentation: Use 2 spaces for indentation.
   - sections: No need to divide into sections, directly output the JSON object.
   - highlighting: No need to highlight.

3. Validation Rules:
   - validation: Use JSON Schema to validate the correctness of the JSON format.
   - constraints: All fields must exist and have the correct type.
   - error_handling: If a valid JSON cannot be generated, output an error message and provide the reason.

4. Example Description:
   1. Example 1:
      - Title: Complete Example
      - Format Type: json
      - Description: Complete example containing all three fields.
      - Example Content: |
          {
            "summary": "The user asked the AI how to make pizza, and the AI provided the steps to make pizza, including preparing the ingredients and baking methods.",
            "grammar": "The dialogue uses imperative sentences to describe the making steps, such as 'Mix flour and water'.",
            "keywords": [
              {
                "word": "pizza",
                "explanation": "An Italian flatbread, usually covered with tomato sauce, cheese, and toppings."
              },
              {
                "word": "bake",
                "explanation": "The process of heating food in an oven."
              }
            ]
          }

          ## Initialization
          As an AI Dialogue Analysis Expert, you must follow the above Rules, execute tasks according to the Workflows, and output according to the JSON output format. The input and output should be in English.

          ##DIALOGUE TO ANALYZE
${finalConversation}

## TASK
Now, analyze the dialogue above and provide the output strictly in the specified JSON format. Do not include any other text, explanations, or markdown formatting like

          `;

      const analysisModel = this.genAI.getGenerativeModel({
        model: 'gemini-1.5-flash',
      });

      const result = await analysisModel.generateContent(finalPrompt);
      const rawResponseText = result.response.text();

      const jsonStartIndex = rawResponseText.indexOf('{');
      const jsonEndIndex = rawResponseText.lastIndexOf('}');
      let analysisResult: DialogueAnalysisResult;
      if (
        jsonStartIndex === -1 ||
        jsonEndIndex === -1 ||
        jsonEndIndex < jsonStartIndex
      ) {
        this.logger.error(
          `‚ùå [${clientId}] Âú®Ê®°ÂûãÂõûÊáâ‰∏≠Êâæ‰∏çÂà∞ÊúâÊïàÁöÑ JSON Áâ©‰ª∂„ÄÇ`,
        );
        throw new Error('Response does not contain a valid JSON object.');
      }

      const jsonString = rawResponseText.substring(
        jsonStartIndex,
        jsonEndIndex + 1,
      );

      try {
        analysisResult = JSON.parse(jsonString);
      } catch (parseError) {
        this.logger.error(
          `‚ùå [${clientId}] Ëß£ÊûêÊ®°ÂûãÂõûÂÇ≥ÁöÑ JSON Â§±Êïó:`,
          parseError,
        );
        this.logger.error(
          `[${clientId}] üìÑ ÁÑ°Ê≥ïËß£ÊûêÁöÑÂéüÂßãÊñáÂ≠ó: ${rawResponseText}`,
        );
        throw new Error('Failed to parse JSON response from the model.');
      }

      this.logger.log(`‚úÖ [${clientId}] ÊàêÂäüÁîüÊàê‰∏¶Ëß£ÊûêÂ∞çË©±ÂàÜÊûêÁµêÊûú„ÄÇ`);
      this.sendToClientById(clientId, 'conversationAnalysisReady', {
        analysis: analysisResult,
      });
      this.logger.log(`üì§ [${clientId}] Â∑≤Â∞áÊúÄÁµÇÂàÜÊûêÁµêÊûúÁôºÈÄÅËá≥ÂâçÁ´Ø„ÄÇ`);
    } catch (error) {
      this.logger.error(
        `‚ùå [${clientId}] Âú®ÊúÄÁµÇÂ∞çË©±ÂàÜÊûêÈÅéÁ®ã‰∏≠ÁôºÁîüÈåØË™§:`,
        error,
      );
      this.sendToClientById(clientId, 'conversationAnalysisFailed', {
        error: 'ÁÑ°Ê≥ïÁîüÊàêÂ∞çË©±ÂàÜÊûêÔºåË´ãÁ®çÂæåÂÜçË©¶„ÄÇ',
      });
    }
  }

  private startElevenLabsStream(clientId: string, onCloseCallback: () => void) {
    const voiceId = 'si0svtk05vPEuvwAW93c';
    const wsUrl = `wss://api.elevenlabs.io/v1/text-to-speech/${voiceId}/stream-input?model_id=eleven_multilingual_v2`;
    const ttsWs = new WebSocket(wsUrl, {
      headers: { 'xi-api-key': process.env.ELEVENLABS_API_KEY },
    });
    this.ttsConnections.set(clientId, ttsWs);

    ttsWs.on('open', () => {
      this.logger.log(`üîä [${clientId}] TTS WebSocket ÈÄ£Á∑öÂ∑≤ÈñãÂïü„ÄÇ`);
    });

    ttsWs.on('message', (data: Buffer) => {
      const response = JSON.parse(data.toString());
      if (response.audio) {
        this.sendToClientById(clientId, 'audioResponse', response.audio);
      } else {
        // this.logger.debug(`[${clientId}] TTS Êî∂Âà∞ÈùûÈü≥Ë®äË®äÊÅØ:`, response);
      }
    });

    ttsWs.on('close', () => {
      this.logger.log(`üîä [${clientId}] TTS WebSocket ÈÄ£Á∑öÂ∑≤ÈóúÈñâ„ÄÇ`);
      this.ttsConnections.delete(clientId);
      onCloseCallback();
    });

    ttsWs.on('error', (error) => {
      this.logger.error(`‚ùå [${clientId}] TTS WebSocket ÁôºÁîüÈåØË™§:`, error);
      this.ttsConnections.get(clientId)?.close();
    });
  }

  private sendToClientById(
    clientId: string,
    event: string,
    payload: any,
  ): boolean {
    const client = this.clientsById.get(clientId);
    if (client && client.readyState === WebSocket.OPEN) {
      client.send(JSON.stringify({ event, payload }));
      return true;
    }
    return false;
  }

  // --- Redis Helpers (‰ΩøÁî®ÊÇ®ÁöÑ RedisService) ---
  private getHistoryRedisKey(clientId: string): string {
    return `history:${clientId}`;
  }
  private async getHistoryFromRedis(clientId: string): Promise<ChatHistory[]> {
    const key = this.getHistoryRedisKey(clientId);
    const historyJson = await this.redisService.get(key);
    return historyJson ? JSON.parse(historyJson) : [];
  }
  private async updateHistoryInRedis(
    clientId: string,
    oldHistory: ChatHistory[],
    userTurn: ChatHistory,
    modelTurn: ChatHistory,
  ) {
    const key = this.getHistoryRedisKey(clientId);
    const newHistory = [...oldHistory, userTurn, modelTurn];
    await this.redisService.set(key, JSON.stringify(newHistory), 60 * 5);
  }
}
